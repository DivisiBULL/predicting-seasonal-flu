{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Business-Case\" data-toc-modified-id=\"Business-Case-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Business Case</a></span></li><li><span><a href=\"#The-Data\" data-toc-modified-id=\"The-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>The Data</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Bringing-in-the-Data\" data-toc-modified-id=\"Bringing-in-the-Data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Bringing in the Data</a></span></li><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>EDA</a></span></li><li><span><a href=\"#Cleaning\" data-toc-modified-id=\"Cleaning-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Cleaning</a></span></li><li><span><a href=\"#Train-Test-Split\" data-toc-modified-id=\"Train-Test-Split-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Train Test Split</a></span></li><li><span><a href=\"#Pipelines\" data-toc-modified-id=\"Pipelines-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Pipelines</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setting-up-column-transformer\" data-toc-modified-id=\"Setting-up-column-transformer-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Setting up column transformer</a></span></li><li><span><a href=\"#Logistic-Regression-Vanilla\" data-toc-modified-id=\"Logistic-Regression-Vanilla-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Logistic Regression Vanilla</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-Report:-Logistic-Regression-Vanilla\" data-toc-modified-id=\"Classification-Report:-Logistic-Regression-Vanilla-9.2.1\"><span class=\"toc-item-num\">9.2.1&nbsp;&nbsp;</span>Classification Report: Logistic Regression Vanilla</a></span></li></ul></li><li><span><a href=\"#Logistic-Regression-GridSearchCV\" data-toc-modified-id=\"Logistic-Regression-GridSearchCV-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Logistic Regression GridSearchCV</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been hired by a **fictitious** version of Costco Pharmacy (**My opinions are my own and in no way reflect that of Costco Pharmacy**). I have been hired by the pharmacy to try and gain a better understanding of what affects personal vaccination rates. Their hope is to try and encourage more of their members to visit the pharmacy for their seasonal flu vaccines. \n",
    "\n",
    "Due to the COVID-19 pandemic vaccination rates have never been such a talking point before. Here in the US the decision to get vaccinated has devolved into a political divide instead of a public health effort.\n",
    "\n",
    "I will be providing Costco Pharmacy with an ability to make a prediction of which of their members have been vaccinated and which have not. To be able to create more targeted advertising to those who have not been vaccinated. This model will also be able to determine which of our features are the most important to someone getting the seasonal flu which will be able to aid in the direction of the marketing team.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will be looking at today comes from another pandemic, the H1N1 influenza pandemic. This data originally comes from a National 2009 H1N1 Flu Survey, which also included information on the seasonal flu. I brought in the data from a competition being hosted by DRIVENDATA. You can find the download page [here](https://www.drivendata.org/competitions/66/flu-shot-learning/data/) (Note: It requires you to sign up and register to the competition to be able to download, but itâ€™s free to do so). You can also visit their benchmark notebook [here](https://drivendata.co/blog/predict-flu-vaccine-data-benchmark/). Which looks at this project as a multi classification problem for both H1N1 and the seasonal flu. I am currently looking at it only trying to predict the vaccination rate for seasonal flu.\n",
    "\n",
    "In the future I hope to look at a similar classification problem with COVID-19 but the data acquisition required for that was outside the scope of my project. This data will still prove useful to our stakeholder to try and gain a better understanding of what affects personal vaccination rates.\n",
    "\n",
    "I would also like to look at data streams directly from Costco Pharmacy. I could try and determine what other factors play important roles in vaccination rates from information they are already collecting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall accuracy is important for this project but the main metric we will be looking at is recall.\n",
    "\n",
    "The marketing team has decided they would prefer to air on the side of caution and still market to those who are possibly already vaccinated (higher recall). Marketing material to those who are already vaccinated might cause minor annoyance but will not be that serious. It might also provide information to those who have been vaccinated for the seasonal flu at a different location that Costco Pharmacy offers the same service. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    plot_confusion_matrix, \n",
    "    roc_auc_score, \n",
    "    classification_report\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "RANDOM_SEED = 42    # Set a random seed for reproducibility\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path.cwd().parent / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv(\n",
    "    DATA_PATH / \"training_set_features.csv\", \n",
    "    index_col=\"respondent_id\"\n",
    ")\n",
    "df_labels = pd.read_csv(\n",
    "    DATA_PATH / \"training_set_labels.csv\", \n",
    "    index_col=\"respondent_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the data cleaning (eg. OHE, imputing NaNs) will happen in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will be dropping features related to h1n1 since we would not have this\n",
    "# information for future data \n",
    "select_features = ['behavioral_antiviral_meds', \n",
    "                   'behavioral_avoidance',\n",
    "                   'behavioral_face_mask', \n",
    "                   'behavioral_wash_hands', \n",
    "                   'behavioral_large_gatherings', \n",
    "                   'behavioral_outside_home', \n",
    "                   'behavioral_touch_face', \n",
    "                   'doctor_recc_seasonal', \n",
    "                   'chronic_med_condition', \n",
    "                   'child_under_6_months', \n",
    "                   'health_worker', \n",
    "                   'health_insurance', \n",
    "                   'opinion_seas_vacc_effective', \n",
    "                   'opinion_seas_risk', \n",
    "                   'opinion_seas_sick_from_vacc', \n",
    "                   'age_group', \n",
    "                   'education', \n",
    "                   'race', \n",
    "                   'sex',\n",
    "                   'income_poverty', \n",
    "                   'marital_status', \n",
    "                   'rent_or_own', \n",
    "                   'employment_status', \n",
    "                   'hhs_geo_region', \n",
    "                   'census_msa', \n",
    "                   'household_adults', \n",
    "                   'household_children', \n",
    "                   'employment_industry', \n",
    "                   'employment_occupation']\n",
    "\n",
    "\n",
    "\n",
    "df_clean = df_features[select_features]\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see we drop six features related to H1N1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X, y\n",
    "y = df_labels['seasonal_vaccine']\n",
    "X = df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up column transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using mode for filling in numeric NaNs because most of my data is binary or an opinion ranked 1-5\n",
    "\n",
    "For categorical data I will simply be adding a missing indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll throw these mini-pipelines into our ColumnTransformer.\n",
    "\n",
    "subpipe_num = Pipeline(steps=[\n",
    "    # Fills in num Nan's with the mode ('most_frequent')\n",
    "    ('num_impute', SimpleImputer(strategy='most_frequent')),\n",
    "    # Scales the data\n",
    "    ('ss', StandardScaler())]\n",
    ")\n",
    "subpipe_cat = Pipeline(steps=[\n",
    "    # Fills in cat Nan's as \"missing\"\n",
    "    ('cat_impute', SimpleImputer(fill_value=\"missing\", strategy=\"constant\")),\n",
    "    # One Hot Encoding\n",
    "    ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the mini pipelines to their respective columns \n",
    "# numeric compiler to numeric columns, cat compiler to cat columns\n",
    "\n",
    "# Selects the respective columns (numeric and categorical)\n",
    "numeric_columns = df_clean.select_dtypes(exclude=object).columns\n",
    "cat_columns = df_clean.select_dtypes(include=object).columns\n",
    "\n",
    "# Applies the transformer\n",
    "CT = ColumnTransformer(transformers=[\n",
    "    ('subpipe_num', subpipe_num, numeric_columns),\n",
    "                                    \n",
    "    ('subpipe_cat', subpipe_cat, cat_columns)],\n",
    "                       \n",
    "                       remainder='passthrough')\n",
    "\n",
    "# The \"remainder='passthrough'\" bit tells the compiler to leave\n",
    "# the other df columns unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr_vanilla = Pipeline(steps=[\n",
    "    ('ct', CT),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr_vanilla.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report: Logistic Regression Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(pipeline_lr_vanilla, X_train, y_train, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_lr_vanilla.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training confusion matrix\n",
    "plot_confusion_matrix(pipeline_lr_vanilla, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training confusion matrix (normalized)\n",
    "plot_confusion_matrix(pipeline_lr_vanilla, X_train, y_train, \n",
    "                      normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test confusion matrix\n",
    "plot_confusion_matrix(pipeline_lr_vanilla, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test confusion matrix (normalized)\n",
    "plot_confusion_matrix(pipeline_lr_vanilla, X_test, y_test,\n",
    "                      normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr_vanilla.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [    \n",
    "    {'lr__penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'lr__C' : np.logspace(-4, 4, 20),\n",
    "    'lr__solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
    "    'lr__max_iter' : [100, 1000,2500, 5000]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lr = GridSearchCV(pipeline_lr_vanilla, param_grid, cv=5, verbose=2)\n",
    "grid_lr.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
